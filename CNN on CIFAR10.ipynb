{
    "metadata": {
        "language_info": {
            "version": "3.5.2", 
            "file_extension": ".py", 
            "codemirror_mode": {
                "version": 3, 
                "name": "ipython"
            }, 
            "mimetype": "text/x-python", 
            "nbconvert_exporter": "python", 
            "name": "python", 
            "pygments_lexer": "ipython3"
        }, 
        "kernelspec": {
            "display_name": "Python 3.5 (Experimental) with Spark 2.0", 
            "language": "python", 
            "name": "python3-spark20"
        }
    }, 
    "nbformat": 4, 
    "cells": [
        {
            "metadata": {}, 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "execution_count": 1, 
                    "data": {
                        "text/plain": "'CIFAR10 '"
                    }
                }
            ], 
            "cell_type": "code", 
            "execution_count": 1, 
            "source": "'CIFAR10 '"
        }, 
        {
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Using TensorFlow backend.\n", 
                    "name": "stderr"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 1, 
            "source": "from __future__ import print_function\nimport keras\nfrom keras.datasets import cifar10\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\n\nimport os\nimport pickle\nimport numpy as np"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 2, 
            "source": "batch_size = 32\nnum_classes = 10\nepochs = 10\ndata_augmentation = True\nnum_predictions = 20\nsave_dir = os.path.join(os.getcwd(), 'saved_models')\nmodel_name = 'keras_cifar10_trained_model.h5'"
        }, 
        {
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "x_train shape: (50000, 32, 32, 3)\n50000 train samples\n10000 test samples\n", 
                    "name": "stdout"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 3, 
            "source": "# The data, shuffled and split between train and test sets:\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')"
        }, 
        {
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 4, 
            "source": "# Convert class vectors to binary class matrices.\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)"
        }, 
        {
            "metadata": {}, 
            "outputs": [
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "execution_count": 5, 
                    "data": {
                        "text/plain": "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n       ..., \n       [ 0.,  0.,  0., ...,  0.,  0.,  1.],\n       [ 0.,  1.,  0., ...,  0.,  0.,  0.],\n       [ 0.,  1.,  0., ...,  0.,  0.,  0.]])"
                    }
                }
            ], 
            "cell_type": "code", 
            "execution_count": 5, 
            "source": "y_train"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 6, 
            "source": "model = Sequential()"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 7, 
            "source": "model.add(Conv2D(32, (3, 3), padding='same',\n                 input_shape=x_train.shape[1:]))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(32, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(64, (3, 3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 8, 
            "source": "# initiate RMSprop optimizer\nopt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 9, 
            "source": "# Let's train the model using RMSprop\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 10, 
            "source": "x_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train /= 255\nx_test /= 255"
        }, 
        {
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Train on 50000 samples, validate on 10000 samples\nEpoch 1/10\n 3680/50000 [=>............................] - ETA: 76s - loss: 2.2887 - acc: 0.1277", 
                    "name": "stdout"
                }, 
                {
                    "output_type": "error", 
                    "traceback": [
                        "\u001b[1;31m\u001b[0m", 
                        "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)", 
                        "\u001b[1;32m<ipython-input-10-85b792961e7e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m               \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m               shuffle=True)\n\u001b[0m", 
                        "\u001b[1;32m/usr/local/src/conda3_runtime.v18/4.1.1/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    868\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 870\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n", 
                        "\u001b[1;32m/usr/local/src/conda3_runtime.v18/4.1.1/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m   1505\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1506\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1507\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1508\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/conda3_runtime.v18/4.1.1/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[0;32m   1154\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1155\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1156\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1157\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1158\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/conda3_runtime.v18/4.1.1/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2267\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2268\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2269\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2270\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/conda3_runtime.v18/4.1.1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    787\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 789\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    790\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/conda3_runtime.v18/4.1.1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    995\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 997\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    998\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/conda3_runtime.v18/4.1.1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1132\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1133\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n", 
                        "\u001b[1;32m/usr/local/src/conda3_runtime.v18/4.1.1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1137\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1138\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1139\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1140\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;32m/usr/local/src/conda3_runtime.v18/4.1.1/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n", 
                        "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
                    ], 
                    "ename": "KeyboardInterrupt", 
                    "evalue": ""
                }
            ], 
            "cell_type": "code", 
            "execution_count": 10, 
            "source": "model.fit(x_train, y_train,\n              batch_size=batch_size,\n              epochs=epochs,\n              validation_data=(x_test, y_test),\n              shuffle=True)"
        }, 
        {
            "metadata": {}, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 11, 
            "source": "# This will do preprocessing and realtime data augmentation:\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images"
        }, 
        {
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 12, 
            "source": "    # Compute quantities required for feature-wise normalization\n    # (std, mean, and principal components if ZCA whitening is applied).\ndatagen.fit(x_train)"
        }, 
        {
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Epoch 1/10\n1562/1562 [==============================] - 76s - loss: 1.9922 - acc: 0.2592 - val_loss: 2.3091 - val_acc: 0.1074\nEpoch 2/10\n1562/1562 [==============================] - 74s - loss: 1.8456 - acc: 0.3216 - val_loss: 2.3058 - val_acc: 0.1275\nEpoch 3/10\n1562/1562 [==============================] - 74s - loss: 1.7832 - acc: 0.3512 - val_loss: 2.3058 - val_acc: 0.1027\nEpoch 4/10\n1562/1562 [==============================] - 74s - loss: 1.7398 - acc: 0.3682 - val_loss: 2.3089 - val_acc: 0.1041\nEpoch 5/10\n1562/1562 [==============================] - 73s - loss: 1.7060 - acc: 0.3838 - val_loss: 2.3182 - val_acc: 0.1149\nEpoch 6/10\n1562/1562 [==============================] - 73s - loss: 1.6782 - acc: 0.3941 - val_loss: 2.3336 - val_acc: 0.1090\nEpoch 7/10\n1562/1562 [==============================] - 73s - loss: 1.6602 - acc: 0.4005 - val_loss: 2.3342 - val_acc: 0.1095\nEpoch 8/10\n1562/1562 [==============================] - 73s - loss: 1.6399 - acc: 0.4081 - val_loss: 2.3577 - val_acc: 0.1020\nEpoch 9/10\n1562/1562 [==============================] - 73s - loss: 1.6275 - acc: 0.4117 - val_loss: 2.4110 - val_acc: 0.1047\nEpoch 10/10\n1562/1562 [==============================] - 74s - loss: 1.6167 - acc: 0.4207 - val_loss: 2.4271 - val_acc: 0.1018\n", 
                    "name": "stdout"
                }, 
                {
                    "metadata": {}, 
                    "output_type": "execute_result", 
                    "execution_count": 13, 
                    "data": {
                        "text/plain": "<keras.callbacks.History at 0x7f5ab04499b0>"
                    }
                }
            ], 
            "cell_type": "code", 
            "execution_count": 13, 
            "source": "    # Fit the model on the batches generated by datagen.flow().\n    model.fit_generator(datagen.flow(x_train, y_train,\n                                     batch_size=batch_size),\n                        steps_per_epoch=x_train.shape[0] // batch_size,\n                        epochs=epochs,\n                        validation_data=(x_test, y_test),\n                        workers=4)"
        }, 
        {
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Saved trained model at /gpfs/global_fs01/sym_shared/YPProdSpark/user/s45f-65f67acefe15fa-238764f4c881/notebook/work/saved_models/keras_cifar10_trained_model.h5 \n", 
                    "name": "stdout"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 16, 
            "source": "# Save model and weights\nif not os.path.isdir(save_dir):\n    os.makedirs(save_dir)\nmodel_path = os.path.join(save_dir, model_name)\nmodel.save(model_path)\nprint('Saved trained model at %s ' % model_path)"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 17, 
            "source": "# Load label names to use in prediction results\nlabel_list_path = 'datasets/cifar-10-batches-py/batches.meta'\n"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 18, 
            "source": "keras_dir = os.path.expanduser(os.path.join('~', '.keras'))\ndatadir_base = os.path.expanduser(keras_dir)\nif not os.access(datadir_base, os.W_OK):\n    datadir_base = os.path.join('/tmp', '.keras')\nlabel_list_path = os.path.join(datadir_base, label_list_path)\n\nwith open(label_list_path, mode='rb') as f:\n    labels = pickle.load(f)"
        }, 
        {
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Model Accuracy = 0.74\n", 
                    "name": "stdout"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 19, 
            "source": "# Evaluate model with test data set and share sample prediction results\nevaluation = model.evaluate_generator(datagen.flow(x_test, y_test,\n                                                   batch_size=batch_size,\n                                                   shuffle=False),\n                                      steps=x_test.shape[0] // batch_size,\n                                      workers=4)\nprint('Model Accuracy = %.2f' % (evaluation[1]))"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": 20, 
            "source": "predict_gen = model.predict_generator(datagen.flow(x_test, y_test,\n                                                   batch_size=batch_size,\n                                                   shuffle=False),\n                                      steps=x_test.shape[0] // batch_size,\n                                      workers=4)"
        }, 
        {
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "text": "Actual Label = cat vs. Predicted Label = frog\nActual Label = ship vs. Predicted Label = bird\nActual Label = ship vs. Predicted Label = ship\nActual Label = airplane vs. Predicted Label = airplane\nActual Label = frog vs. Predicted Label = cat\nActual Label = frog vs. Predicted Label = horse\nActual Label = automobile vs. Predicted Label = bird\nActual Label = frog vs. Predicted Label = frog\nActual Label = cat vs. Predicted Label = ship\nActual Label = automobile vs. Predicted Label = ship\nActual Label = airplane vs. Predicted Label = airplane\nActual Label = truck vs. Predicted Label = bird\nActual Label = dog vs. Predicted Label = truck\nActual Label = horse vs. Predicted Label = cat\nActual Label = truck vs. Predicted Label = dog\nActual Label = ship vs. Predicted Label = ship\nActual Label = dog vs. Predicted Label = ship\nActual Label = horse vs. Predicted Label = automobile\nActual Label = ship vs. Predicted Label = automobile\nActual Label = frog vs. Predicted Label = horse\nActual Label = horse vs. Predicted Label = bird\n", 
                    "name": "stdout"
                }
            ], 
            "cell_type": "code", 
            "execution_count": 21, 
            "source": "for predict_index, predicted_y in enumerate(predict_gen):\n    actual_label = labels['label_names'][np.argmax(y_test[predict_index])]\n    predicted_label = labels['label_names'][np.argmax(predicted_y)]\n    print('Actual Label = %s vs. Predicted Label = %s' % (actual_label,\n                                                          predicted_label))\n    if predict_index == num_predictions:\n        break"
        }, 
        {
            "metadata": {
                "collapsed": true
            }, 
            "outputs": [], 
            "cell_type": "code", 
            "execution_count": null, 
            "source": ""
        }
    ], 
    "nbformat_minor": 1
}